{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking the Features, Part 2\n",
    "\n",
    "Our first round of feature tweaks brought us a certain set of features, but I also want to test the influence of the number of values collapsed into the Other value for certain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_df = pd.read_csv(\"Data/charity_data.csv\")[[\"APPLICATION_TYPE\", \"AFFILIATION\", \"CLASSIFICATION\", \"ORGANIZATION\", \"INCOME_AMT\", \"IS_SUCCESSFUL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Independent         18480\n",
       "CompanySponsored    15705\n",
       "Family/Parent          64\n",
       "National               33\n",
       "Regional               13\n",
       "Other                   4\n",
       "Name: AFFILIATION, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df['AFFILIATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "C1270      114\n",
       "C2700      104\n",
       "C2800       95\n",
       "C7100       75\n",
       "C1300       58\n",
       "C1280       50\n",
       "C1230       36\n",
       "C1400       34\n",
       "C7200       32\n",
       "C2300       32\n",
       "C1240       30\n",
       "C8000       20\n",
       "C7120       18\n",
       "C1500       16\n",
       "C1800       15\n",
       "C6000       15\n",
       "C1250       14\n",
       "C8200       11\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = application_df['CLASSIFICATION'].value_counts()\n",
    "temp[temp > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trust           23515\n",
       "Association     10255\n",
       "Co-operative      486\n",
       "Corporation        43\n",
       "Name: ORGANIZATION, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df['ORGANIZATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                24388\n",
       "25000-99999       3747\n",
       "100000-499999     3374\n",
       "1M-5M              955\n",
       "1-9999             728\n",
       "10000-24999        543\n",
       "10M-50M            240\n",
       "5M-10M             185\n",
       "50M+               139\n",
       "Name: INCOME_AMT, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "application_df['INCOME_AMT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only generate for APPLICATION_TYPE, CLASSIFICATION, and INCOME_AMT\n",
    "# because the others were rather obvious where the outliers start\n",
    "def get_column_counts():\n",
    "    for app_type_count in range(5,9):\n",
    "        for classify_count in range(5,8):\n",
    "            for inc_count in range(3,5):\n",
    "                yield (app_type_count, classify_count, inc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_app_df(col_counts):\n",
    "    application_df = pd.read_csv(\"Data/charity_data.csv\")[[\"APPLICATION_TYPE\", \"AFFILIATION\", \"CLASSIFICATION\", \"ORGANIZATION\", \"INCOME_AMT\", \"IS_SUCCESSFUL\"]]\n",
    "    # Choose a cutoff value and create a list of application types to be replaced\n",
    "    for app in application_df['APPLICATION_TYPE'].value_counts().index[col_counts[0]:]:\n",
    "        application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "    # Choose a cutoff value and create a list of affiliations to be replaced\n",
    "    for app in application_df['AFFILIATION'].value_counts().index[2:]:\n",
    "        application_df['AFFILIATION'] = application_df['AFFILIATION'].replace(app,\"Other\")\n",
    "\n",
    "    # Choose a cutoff value and create a list of classifications to be replaced\n",
    "    for cls in application_df['CLASSIFICATION'].value_counts().index[col_counts[1]:]:\n",
    "        application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "\n",
    "    # Choose a cutoff value and create a list of organizations to be replaced\n",
    "    for app in application_df['ORGANIZATION'].value_counts().index[2:]:\n",
    "        application_df['ORGANIZATION'] = application_df['ORGANIZATION'].replace(app,\"Other\")\n",
    "\n",
    "    # Choose a cutoff value and create a list of income amounts to be replaced\n",
    "    for inc in application_df['INCOME_AMT'].value_counts().index[col_counts[2]:]:\n",
    "        application_df['INCOME_AMT'] = application_df['INCOME_AMT'].replace(inc,\"Other\")\n",
    "\n",
    "    return pd.get_dummies(application_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_cols):\n",
    "    # Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "    # Use most efficient neural network from tuning notebook. Won't always be most efficient, but it gives us a chance\n",
    "    # 0 - selu 6, 1 - selu 13, 2 - tanh 7, output - sigmoid 1\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units=6, activation=\"selu\", input_dim=num_cols))\n",
    "    model.add(tf.keras.layers.Dense(units=13, activation=\"selu\"))\n",
    "    model.add(tf.keras.layers.Dense(units=7, activation=\"tanh\"))\n",
    "    model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 3)\n",
      "268/268 - 0s - loss: 0.5618 - accuracy: 0.7221 - 409ms/epoch - 2ms/step\n",
      "(5, 5, 4)\n",
      "268/268 - 0s - loss: 0.5628 - accuracy: 0.7234 - 375ms/epoch - 1ms/step\n",
      "(5, 6, 3)\n",
      "268/268 - 0s - loss: 0.5629 - accuracy: 0.7186 - 377ms/epoch - 1ms/step\n",
      "(5, 6, 4)\n",
      "268/268 - 0s - loss: 0.5621 - accuracy: 0.7237 - 378ms/epoch - 1ms/step\n",
      "(5, 7, 3)\n",
      "268/268 - 0s - loss: 0.5642 - accuracy: 0.7212 - 376ms/epoch - 1ms/step\n",
      "(5, 7, 4)\n",
      "268/268 - 0s - loss: 0.5641 - accuracy: 0.7178 - 400ms/epoch - 1ms/step\n",
      "(6, 5, 3)\n",
      "268/268 - 0s - loss: 0.5634 - accuracy: 0.7226 - 394ms/epoch - 1ms/step\n",
      "(6, 5, 4)\n",
      "268/268 - 0s - loss: 0.5606 - accuracy: 0.7256 - 391ms/epoch - 1ms/step\n",
      "(6, 6, 3)\n",
      "268/268 - 0s - loss: 0.5623 - accuracy: 0.7237 - 380ms/epoch - 1ms/step\n",
      "(6, 6, 4)\n",
      "268/268 - 0s - loss: 0.5620 - accuracy: 0.7215 - 378ms/epoch - 1ms/step\n",
      "(6, 7, 3)\n",
      "268/268 - 0s - loss: 0.5626 - accuracy: 0.7224 - 376ms/epoch - 1ms/step\n",
      "(6, 7, 4)\n",
      "268/268 - 0s - loss: 0.5602 - accuracy: 0.7265 - 383ms/epoch - 1ms/step\n",
      "(7, 5, 3)\n",
      "268/268 - 0s - loss: 0.5593 - accuracy: 0.7278 - 378ms/epoch - 1ms/step\n",
      "(7, 5, 4)\n",
      "268/268 - 0s - loss: 0.5584 - accuracy: 0.7276 - 372ms/epoch - 1ms/step\n",
      "(7, 6, 3)\n",
      "268/268 - 0s - loss: 0.5593 - accuracy: 0.7280 - 375ms/epoch - 1ms/step\n",
      "(7, 6, 4)\n",
      "268/268 - 0s - loss: 0.5572 - accuracy: 0.7284 - 373ms/epoch - 1ms/step\n",
      "(7, 7, 3)\n",
      "268/268 - 0s - loss: 0.5589 - accuracy: 0.7277 - 377ms/epoch - 1ms/step\n",
      "(7, 7, 4)\n",
      "268/268 - 0s - loss: 0.5587 - accuracy: 0.7270 - 375ms/epoch - 1ms/step\n",
      "(8, 5, 3)\n",
      "268/268 - 0s - loss: 0.5594 - accuracy: 0.7297 - 378ms/epoch - 1ms/step\n",
      "(8, 5, 4)\n",
      "268/268 - 0s - loss: 0.5586 - accuracy: 0.7254 - 376ms/epoch - 1ms/step\n",
      "(8, 6, 3)\n",
      "268/268 - 0s - loss: 0.5592 - accuracy: 0.7241 - 379ms/epoch - 1ms/step\n",
      "(8, 6, 4)\n",
      "268/268 - 0s - loss: 0.5576 - accuracy: 0.7272 - 378ms/epoch - 1ms/step\n",
      "(8, 7, 3)\n",
      "268/268 - 0s - loss: 0.5577 - accuracy: 0.7276 - 385ms/epoch - 1ms/step\n",
      "(8, 7, 4)\n",
      "268/268 - 0s - loss: 0.5587 - accuracy: 0.7280 - 382ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Beware, this can take almost 6 hours to run!\n",
    "all_model_stats = []\n",
    "\n",
    "for col_counts in get_column_counts():\n",
    "    application_df = create_app_df(col_counts)\n",
    "\n",
    "    model_summary = { 'counts': col_counts }\n",
    "\n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "    features = application_df.drop(\"IS_SUCCESSFUL\", axis=1)\n",
    "    target = application_df[\"IS_SUCCESSFUL\"]\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1)\n",
    "\n",
    "    # Create a StandardScaler instance\n",
    "    # Fit the StandardScaler\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    model = create_model(len(features.columns))\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        epochs=34,\n",
    "        verbose=0\n",
    "        )\n",
    "\n",
    "    print(col_counts)\n",
    "    model_loss, model_accuracy = model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "    model_summary[\"accuracy\"] = model_accuracy\n",
    "    model_summary[\"loss\"] = model_loss\n",
    "\n",
    "    all_model_stats.append(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 1 most effective\n",
      "Counts: (8, 5, 3)\n",
      "0.72967928647995\n",
      "0.5594186186790466\n",
      "--------------\n",
      "Number 2 most effective\n",
      "Counts: (7, 6, 4)\n",
      "0.728396475315094\n",
      "0.5572439432144165\n",
      "--------------\n",
      "Number 3 most effective\n",
      "Counts: (7, 6, 3)\n",
      "0.7280466556549072\n",
      "0.5593292713165283\n",
      "--------------\n",
      "Number 4 most effective\n",
      "Counts: (8, 7, 4)\n",
      "0.7280466556549072\n",
      "0.5587282776832581\n",
      "--------------\n",
      "Number 5 most effective\n",
      "Counts: (7, 5, 3)\n",
      "0.7278134226799011\n",
      "0.5592970252037048\n",
      "--------------\n",
      "Number 6 most effective\n",
      "Counts: (7, 7, 3)\n",
      "0.7276967763900757\n",
      "0.558948278427124\n",
      "--------------\n",
      "Number 7 most effective\n",
      "Counts: (7, 5, 4)\n",
      "0.727580189704895\n",
      "0.5584208965301514\n",
      "--------------\n",
      "Number 8 most effective\n",
      "Counts: (8, 7, 3)\n",
      "0.727580189704895\n",
      "0.5576977729797363\n",
      "--------------\n",
      "Number 9 most effective\n",
      "Counts: (8, 6, 4)\n",
      "0.7272303104400635\n",
      "0.5575675368309021\n",
      "--------------\n",
      "Number 10 most effective\n",
      "Counts: (7, 7, 4)\n",
      "0.7269970774650574\n",
      "0.5587436556816101\n",
      "--------------\n",
      "Number 11 most effective\n",
      "Counts: (6, 7, 4)\n",
      "0.7265306115150452\n",
      "0.5602442622184753\n",
      "--------------\n",
      "Number 12 most effective\n",
      "Counts: (6, 5, 4)\n",
      "0.7255976796150208\n",
      "0.5606220364570618\n",
      "--------------\n",
      "Number 13 most effective\n",
      "Counts: (8, 5, 4)\n",
      "0.7253644466400146\n",
      "0.5585663318634033\n",
      "--------------\n",
      "Number 14 most effective\n",
      "Counts: (8, 6, 3)\n",
      "0.7240816354751587\n",
      "0.5592413544654846\n",
      "--------------\n",
      "Number 15 most effective\n",
      "Counts: (5, 6, 4)\n",
      "0.7237317562103271\n",
      "0.5621249675750732\n",
      "--------------\n",
      "Number 16 most effective\n",
      "Counts: (6, 6, 3)\n",
      "0.7237317562103271\n",
      "0.5622538924217224\n",
      "--------------\n",
      "Number 17 most effective\n",
      "Counts: (5, 5, 4)\n",
      "0.7233819365501404\n",
      "0.5628435611724854\n",
      "--------------\n",
      "Number 18 most effective\n",
      "Counts: (6, 5, 3)\n",
      "0.7225655913352966\n",
      "0.5633888244628906\n",
      "--------------\n",
      "Number 19 most effective\n",
      "Counts: (6, 7, 3)\n",
      "0.722449004650116\n",
      "0.5626493096351624\n",
      "--------------\n",
      "Number 20 most effective\n",
      "Counts: (5, 5, 3)\n",
      "0.7220991253852844\n",
      "0.5618010759353638\n",
      "--------------\n",
      "Number 21 most effective\n",
      "Counts: (6, 6, 4)\n",
      "0.7215160131454468\n",
      "0.5619595646858215\n",
      "--------------\n",
      "Number 22 most effective\n",
      "Counts: (5, 7, 3)\n",
      "0.72116619348526\n",
      "0.5641812682151794\n",
      "--------------\n",
      "Number 23 most effective\n",
      "Counts: (5, 6, 3)\n",
      "0.7186005711555481\n",
      "0.5629380941390991\n",
      "--------------\n",
      "Number 24 most effective\n",
      "Counts: (5, 7, 4)\n",
      "0.7177842855453491\n",
      "0.5640631914138794\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "# Print all summaries, in order of descending accuracy\n",
    "for i, model_stats in enumerate(sorted(all_model_stats, key=lambda x: x[\"accuracy\"], reverse=True)):\n",
    "    print(f'Number {i+1} most effective\\nCounts: {model_stats[\"counts\"]}\\n{model_stats[\"accuracy\"]}\\n{model_stats[\"loss\"]}\\n--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Given that there was very little appreciable difference between any of these permutations, I think we can safely say that where the line is drawn for outliers doesn't matter much for these features. When you include the results of FeatureTweaks done prior to this, and the fact that its most efficient accuracies were within random-chance tolerances of these results, I don't think it that most of the outliers really affect the training chances at all.\n",
    "\n",
    "Still, I'll use the counts from the number 1 most effective here, if only because they were the cutoffs that I had expected would produce the best results.\n",
    "\n",
    "We're still sitting below our target accuracy, though, so in absence of other ideas, I'll perform a round of more intensive tuning in AlphabetSoupCharity_TuningPt2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
